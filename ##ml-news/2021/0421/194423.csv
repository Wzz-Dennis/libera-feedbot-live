feed,title,long_url,short_url
r/ML:100+,[R] Rotary Positional Embeddings - a new relative positional embedding for Transformers that significantly improves convergence (20-30%) and works for both regular and efficient attention,https://redd.it/mvf7ho,
