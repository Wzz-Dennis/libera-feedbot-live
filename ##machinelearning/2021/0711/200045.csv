feed,title,long_url,short_url
PwC:Trending,/google-research/ ByT5: Towards a token-free future with pre-trained byte-to-byte models: https://github.com/google-research/byt5,https://paperswithcode.com/paper/byt5-towards-a-token-free-future-with-pre,https://j.mp/3vED1hs
PwC:Trending,/ShannonAI/ ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information: https://github.com/ShannonAI/ChineseBert,https://paperswithcode.com/paper/chinesebert-chinese-pretraining-enhanced-by,https://j.mp/3AgB5ic
