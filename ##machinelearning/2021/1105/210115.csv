feed,title,long_url,short_url
r/ML:100+,[P] optimization of Hugging Face Transformer models to get Inference < 1 Millisecond Latency + deployment on production ready inference server,https://redd.it/qn8com,
r/ML:100+,[R] Hierarchical Transformers Are More Efficient Language Models,https://redd.it/qmm9z7,
