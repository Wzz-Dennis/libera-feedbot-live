feed,title,long_url,short_url
r/ML:50+,[R] Pruning Attention Heads of Transformer Models Using A* Search: A Novel Approach to Compress Big NLP Architectures,https://redd.it/qtl5fx,
