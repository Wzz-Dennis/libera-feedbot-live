feed,title,long_url,short_url
PwC:Latest,/wdayang/ CTformer: Convolution-free Token2Token Dilated Vision Transformer for Low-dose CT Denoising: https://github.com/wdayang/ctformer,https://paperswithcode.com/paper/ctformer-convolution-free-token2token-dilated,https://bit.ly/35KKOCE
PwC:Latest,/frostbitexsw/ Towards Robust Stacked Capsule Autoencoder with Hybrid Adversarial Training: https://github.com/frostbitexsw/scae_defense,https://paperswithcode.com/paper/towards-robust-stacked-capsule-autoencoder,https://bit.ly/3vp6LBU
PwC:Latest,/aravindsankar28/ Sparsity-aware neural user behavior modeling in online interaction platforms: https://github.com/aravindsankar28/Inf-VAE,https://paperswithcode.com/paper/sparsity-aware-neural-user-behavior-modeling,https://bit.ly/3ptUoAK
PwC:Latest,/nicklucche/ Avalanche RL: a Continual Reinforcement Learning Library: https://github.com/nicklucche/continual-habitat-lab,https://paperswithcode.com/paper/avalanche-rl-a-continual-reinforcement,https://bit.ly/347cLnC
PwC:Latest,/deepmind/ Evaluating High-Order Predictive Distributions in Deep Learning: https://github.com/deepmind/neural_testbed,https://paperswithcode.com/paper/evaluating-high-order-predictive,https://bit.ly/3IAI4WU
PwC:Latest,/pjbenard/ Fast off-the-grid sparse recovery with over-parametrized projected gradient descent: https://github.com/pjbenard/opcomp_sparse_recovery,https://paperswithcode.com/paper/fast-off-the-grid-sparse-recovery-with-over,https://bit.ly/3MdNucz
PwC:Latest,/microsoft/ ICASSP 2022 Acoustic Echo Cancellation Challenge: https://github.com/microsoft/AEC-Challenge,https://paperswithcode.com/paper/icassp-2022-acoustic-echo-cancellation,https://bit.ly/3vzfgu6
PwC:Latest,/microsoft/ ICASSP 2022 Deep Noise Suppression Challenge: https://github.com/microsoft/DNS-Challenge,https://paperswithcode.com/paper/icassp-2022-deep-noise-suppression-challenge,https://bit.ly/3vuZfpg
PwC:Latest,/ruipingl/ Transformer-based Knowledge Distillation for Efficient Semantic Segmentation of Road-driving Scenes: https://github.com/ruipingl/skr_pea,https://paperswithcode.com/paper/transformer-based-knowledge-distillation-for,https://bit.ly/3IGnWTq
